# Create dataframe frequency tables
freqTable <- data.frame(gram=dimnames(gramCount)[[1]],count=gramCount,stringsAsFactors=FALSE)
gc()
library(tm)
triTDM<-removeSparseTerms(triTDM,0.96)
# Get total frequency in corpus of each trigram
library(slam)
gramCount<-as.matrix(row_sums(triTDM))
# Create dataframe frequency tables
freqTable <- data.frame(gram=dimnames(gramCount)[[1]],count=gramCount,stringsAsFactors=FALSE)
# sort descending the frequency tables
freqTable<-freqTable[order(-freqTable$count),]
# Split corpus trigrams up to words
words <- strsplit(freqTable$gram," ")
# Set first two words as an attribute, the trigram prediction query pair of words
freqTable$triquery <- sapply(words,FUN=function(x) paste(x[1],x[2]))
# Set each word of trigram as an attribute for future use
freqTable$one <- sapply(words,FUN=function(x) paste(x[1]))
freqTable$two <- sapply(words,FUN=function(x) paste(x[2]))
freqTable$three <- sapply(words,FUN=function(x) paste(x[3]))
# 3. INPUT MUNGING
#=============================================#
## INPUT MUNGING ##
# i. Take an input:
input<-list("The guy in front of me just bought a pound of bacon, a bouquet, and a case of","You're the reason why I smile everyday. Can you follow me please? It would mean the","Hey sunshine, can you follow me and make me the","Very early observations on the Bills game: Offense still struggling but the","Go on a romantic date at the","Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my","Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some","After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little","Be grateful for the good times and keep the faith during the","If this isn't the cutest thing you've ever seen, then you must be")
# ii. Perform Transformations.
input<-makeCorpus(input)
input<-lapply(input, FUN=function(x){as.character(x[1])})
# iii. Reduce to last two words
input<-lapply(input,FUN=function(x){unlist(strsplit(x,"\\s+"))})
two<-lapply(input,FUN=length)
one<-lapply(two,FUN=function(x){x-1})
# iv. set querying bigrams and unigrams we will search for in trigrams and bigrams respectively
bigram<-list(lapply(1:length(input),FUN=function(x){paste(input[[x]][one[[x]]],input[[x]][two[[x]]])}))
bigram<-bigram[[1]]
unigram<-list(lapply(1:length(input),FUN=function(x){paste(input[[x]][two[[x]]])}))
unigram<-unigram[[1]]
# 4. FIND PREDICTION MATCHES
#=============================================#
#TRIGRAMS:
# Find trigrams where first two words match and put in matches list
trimatches<-lapply(1:length(bigram), FUN=function(x){freqTable[freqTable$triquery == bigram[[x]],]})
bimatch1 <- lapply(1:length(unigram), FUN=function(x){freqTable[freqTable$one == unigram[[x]],]})
bimatch2 <- lapply(1:length(unigram), FUN=function(x){freqTable[freqTable$two == unigram[[x]],]})
# Put these results in a frequency table and rank them as such.
matches<-lapply(1:length(input), FUN=function(x){c(trimatches[[x]]$three,bimatch1[[x]]$two,bimatch2[[x]]$one)})
matchCorpus<-lapply(1:length(matches),FUN=function(x){makeCorpus(matches[[x]])}) # (Corpus(VectorSource(matches)) caused very similar terms to be considered separately like singular and plural)
matchTDM<-lapply(1:length(matchCorpus),FUN=function(x){TermDocumentMatrix(matchCorpus[[x]])})
# Get total frequency in prediction corpus of each prediction
predCount<-lapply(1:length(matchTDM),FUN=function(x){rowSums(as.matrix(matchTDM[[x]]))})
# Create dataframe frequency table
predFreq <- lapply(1:length(predCount),FUN=function(x){data.frame(gram=names(predCount[[x]]),count=predCount[[x]],stringsAsFactors=FALSE)})
predFreq<-lapply(1:length(predFreq),FUN=function(x){predFreq[[x]][order(-predFreq[[x]]$count),]})
# If predictions are more than 10, reduce to 10
predFreq<-lapply(1:length(predFreq),FUN=function(x){if(nrow(predFreq[[x]])>10){predFreq[[x]]<-predFreq[[x]][1:10,]}})
sink('predictions.txt')
predFreq
sink()
sink('trimatches.txt')
trimatches
sink()
gc()
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/")
# FUNCTION DEFINITIONS #
library(tm)
# Make Corpus and do transformations only
makeCorpus<- function(x) {
corpus<-Corpus(VectorSource(x))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
#corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus<- tm_map(corpus,removePunctuation)
corpus<- tm_map(corpus,removeNumbers)
return(corpus)
}
# TrigramTokenizer function
library(RWeka)
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
# Make Corpus, Transform, Make Trigram TDM
makeTriTDM <- function(x) {
corpus<-Corpus(VectorSource(x))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
#corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus<- tm_map(corpus,removePunctuation)
corpus<- tm_map(corpus,removeNumbers)
tdm<- TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
#tdm<-removeSparseTerms(tdm,0.97)
return(tdm)}
## DATA MUNGING ##
# 1. Corpus, transformations, and TDM Creation
#=============================================#
fileMunge<- function(x) {
text<-readLines(x)
totalLines=length(text)
chunkSize=20000
chunks=totalLines/chunkSize
remainder = chunks %% 1
wholeChunks = chunks-remainder
# initialize list
output=list()
# break file into chunks
i=1
line=1
while (i<=wholeChunks){
end=line+chunkSize-1
output[[i]]<-text[line:end]
line=end+1
i=i+1
}
output[[i]]<-text[line:totalLines]
# Text Transformations to remove odd characters #
output=lapply(output,FUN=iconv, to='ASCII', sub=' ')
output=lapply(output,FUN= function(x) gsub("'{2}", " ",x))
output=lapply(output,FUN= function(x) gsub("[0-9]", " ",x))
}
# Read, chunk, parse data, then make corpus, do transformations, make TDM of tri-grams:
test<-fileMunge("testData1.txt")
triTDM <- makeTriTDM(test)
inspect(triTDM)
findAssocs(triTDM,"are",0)
findAssocs(triTDM,"are",1)
findAssocs(triTDM,"Talking heads are my favorite band",0)
findAssocs(triTDM,"Talk head are",0)
findAssocs(triTDM,"talk head are",0)
fileMunge<- function(x) {
text<-readLines(x)
totalLines=length(text)
chunkSize=1
chunks=totalLines/chunkSize
remainder = chunks %% 1
wholeChunks = chunks-remainder
# initialize list
output=list()
# break file into chunks
i=1
line=1
while (i<=wholeChunks){
end=line+chunkSize-1
output[[i]]<-text[line:end]
line=end+1
i=i+1
}
output[[i]]<-text[line:totalLines]
# Text Transformations to remove odd characters #
output=lapply(output,FUN=iconv, to='ASCII', sub=' ')
output=lapply(output,FUN= function(x) gsub("'{2}", " ",x))
output=lapply(output,FUN= function(x) gsub("[0-9]", " ",x))
}
# Read, chunk, parse data, then make corpus, do transformations, make TDM of tri-grams:
test<-fileMunge("testData1.txt")
triTDM <- makeTriTDM(test)
# rm(test)
# gc()
#1. Calculate probabilities of each line in my testdata1.txt
# "Talking heads are my favorite band"
findAssocs(triTDM,"Talking heads are my favorite band",0)
findAssocs(triTDM,"talk head are",0)
findAssocs(triTDM,"talk head are my",0)
corp<-makeCorpus(test)
corp[1]
corp[[1]
]
corp[[1]][1]
typeof(corp[[1]][1])
typeof(corp[[1]][1][1])
typeof(corp$content[[1]][1])
corp$content[[1]][1]
corp$content[1]
corp$content[[1]]
findAssocs(triTDM,corp[[1]],0)
as.character(corp[[1]])
findAssocs(triTDM,as.character(corp[[1]]),0)
TrigramTokenizer(corp[[1]])
train<-TrigramTokenizer(corp[[1]])
train
train[1]
findAssocs(triTDM,train,0)
findAssocs(triTDM,train[1],0)
typeof(train)
list(train)
findAssocs(triTDM,list(train),0)
trainGrams<-lapply(train,FUN=function(x){findAssocs(triTDM,train[x],0)}
)
trainGrams
trainGrams<-lapply(train,FUN=function(x){findAssocs(triTDM,x,0)})
trainGrams
triTDM
row_sums(triTDM)
library(slam)
row_sums(triTDM)
row_sums(triTDM)[1]
Matrix(test)
?Matrix
triTDM[1]
triTDM[[1]]
summary(triTDM)
triTDM
triTDM$Terms
triTDM[1]$Terms
attributes(triTDM)
names(triTDM)
names(triTDM$dimnames)
names(triTDM$dimnames$Terms)
names(triTDM$dimnames[1])
names(triTDM$dimnames[1][1])
names(triTDM$dimnames[[1]])
names(triTDM$dimnames[1])
triTDM$dimnames[1]
triTDM$dimnames[1][1]
triTDM$dimnames[[1]]
triTDM$dimnames[[1]][2]
triTDM[1,]
triTDM[2,]
?data.frame
triTDM$dimnames[[1]]
typeof(triTDM$dimnames[[1]])
freq<-data.frame(trigrams=triTDM$dimnames[[1]])
freq
freq[1]
dim(freq)
findFreqTerms(triTDM)
?findFreqTerms
inspect(triTDM)
typeof(inspect(triTDM))
inspect(triTDM)[1]
inspect(triTDM)[2]
inspect(triTDM)[[1]]
inspect(triTDM$terms)
inspect(triTDM$Terms)
inspect(triTDM)
dimnames(triTDM)
dimnames(triTDM)$Docs
tdm["are my favorite"]
tdm["are my favorit"]
triTDM["are my favorit"]
triTDM["are my favorit",]
triTDM["are my favorit",dimnames(triTDM)$Docs]
inspect(triTDM)
z<-inspect(triTDM)
typeof(z)
rowSums(z)
rowSums(z)[1]
rowSums(z)["are my favorit"]
rowSums(z)["band are playing"]
z
rowSums(inspect(triTDM))
freq<-data.frame(trigrams=triTDM$dimnames[[1]], count=rowSums(inspect(triTDM))
)
freq
freq<-data.frame(trigrams=triTDM$dimnames[[1]], count=rowSums(inspect(triTDM))
)
freq
rowSums(inspect(triTDM)
)
nrows(freq)
nrow(freq)
freq$MLE<-(freq$count/nrow(freq))
freq
product=c(1:10)
product
sum(product)
data.frame(Freq=product)
df<-data.frame(Freq=product)
df
sum(df$Freq)
tavg=(1/product)
data<-data.frame(names=c("Twitter Corpus", "News Corpus", "Blogs Corpus"), counts=c(1,2,3))
par(mai=c(1,2,1,1))
barplot(data$count, main="Number of Lines per Corpus", horiz=TRUE, names.arg=data$names, las=1)
data
barplot(data, main="Average Unique Word Length per Corpus", horiz=TRUE, names.arg=data$names, las=1)
barplot(data$count, main="Average Unique Word Length per Corpus", horiz=TRUE, names.arg=data$names, las=1)
barplot(data$count, main="Average Unique Word Length per Corpus", horiz=TRUE, names.arg=data$names, las=1)
data
data<-data.frame(names=c("Twitter Corpus", "News Corpus", "Blogs Corpus"), counts=c(3:1))
par(mai=c(1,2,1,1))
barplot(data$counts, main="Average Unique Word Length per Corpus", horiz=TRUE, names.arg=data$names, las=1)
barplot(data$count, main="Average Unique Word Length per Corpus", horiz=TRUE, names.arg=data$names, las=1)
par()
load("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US/triblogs only.TDMloaded.RData")
counts=row_sums(b.tdm)
library(slam)
counts=row_sums(b.tdm)
counts[1]
counts[2]
counts[200]
counts[2001]
triblog<-data.frame(grams=names(counts), counts=counts))
triblog<-data.frame(grams=names(counts), counts=counts)
View(triblog)
triblog[1,]
object.size(triblog)
rm(triblog)
gc()
which(names(counts)=="escort")
names(counts[2])
names(counts[3])
which(names(counts)=="a a a")
?grep
grep("a a",counts)
grep("a a",names(counts))
grep("a a ",names(counts))
a_a<-counts[grep("a a ",names(counts)),]
dim(counts)
rows<-grep("a a ",names(counts))
results<-data.frame(names=names(counts[rows,]),counts=counts[rows,])
results<-data.frame(names=names(counts[rows]),counts=counts[rows])
View(results)
predict<-function(x){
rows<-grep(x,names(counts))
results<-data.frame(names=names(counts[rows]),counts=counts[rows])
}
predict("a a")
predict("at the")
View(results)
results<-predict("at the")
View(results)
results<-predict(" at the ")
View(results)
results<-predict("at the")
View(results)
results<-predict(" at the")
results<-predict("at the ")
View(results)
View(results)
predict<-function(x){
rows<-grep(x,names(counts))
results<-data.frame(names=names(counts[rows]),counts=counts[rows])
results<-results[order(-results$counts),]
}
results<-predict("at the ")
View(results)
results2<-predict("at the")
object.size(results2)
View(results2)
results<-predict("at the beach")
View(results)
results<-predict("at the mall")
View(results)
results<-predict("at the movies")
View(results)
results<-predict("at the movi")
View(results)
results<-predict("on my ")
View(results)
rm(results2)
rm(b.tdm,b.dat)
rm(ptm)
save.image("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US/triblogs only.2.countsOnlyandPredictReady..RData")
results<-predict("must be ")
View(results)
results<-predict("must be insane")
View(results)
gc()
saveRDS(counts,"blogTriCounts")
library(ff)
my.obg<-ff(counts)
my.obg
?ff
ffBTcounts<-ff(counts,filename=ffBTcounts)
ffBlogs<-ffdf(tri=counts)
Counts<-ff(counts)
ffBlogs<-ffdf(tri=Counts)
rm(my.obg)
ffBlogs
ffBlogs
dim(ffBlogs)
ffsave(ffBlogs)
ffsave(ffBlogs,file=ffBlogs)
ffsave(ffBlogs,file="ffBlogs")
ffsave(ffBlogs,file="Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-Swiftkey/final/en_US/ffBlogs")
ffBlogs$tri
ffBlogs$tri[1]
ffBlogs$tri[2]
ffBlogs$tri[200]
ffBlogs[1,]
ffBlogs<-ffdf(names=names(Counts),freq=Counts)
names(Counts)
names(counts)
glob2rx("blue*")
?grob2rx
?glob2rx
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/Health2")
shiny::runApp()
###Pre-Processing
YTD <- read.csv("1B.csv",stringsAsFactors=FALSE)
YTD <-YTD[-1:-4,]
YTD2<-YTD[,c(1,6:11,13:15)]
YTD2=apply(YTD2, c(1,2), function(x) gsub('\\%', '', x))
YTD2=apply(YTD2, c(1,2), function(x) gsub('\\$', '', x))
YTD2=data.frame(YTD2)
#Row and column names
colnames(YTD2)<-as.character(c("Country/Region","Full Year OP ($$)","YTD OP ($$)","YTD Actuals ($$)","% to OP (%)","Meets OI Requirements", "YTD Incremental over OP ($$)", "% Local Growth OP", "% Local Growth Actual", "LCG % pts. over OP"))
#Filter
YTD2$"sort1" <- as.numeric(sub("\\$","", YTD2$"Full Year OP ($$)"))
YTD3<- subset(YTD2,sort1>=7)
YTD4<-subset(YTD3,YTD3$"Meets OI Requirements"=="YES")
#Cut to YTD columns
YTD5<-YTD4[,c(1:5,7)]
#Make numeric sort column, sort, erase sort column
YTD5$"sort2" <- as.numeric(sub("\\$","", YTD5$"YTD Incremental over OP ($$)"))
YTD6<-YTD5[order(-YTD5$sort2),]
rownames(YTD6)<-c(1:dim(YTD6)[1])
YTD7<-cbind("Rank"=rownames(YTD6),YTD6[,1:6])
#YTD Table ready.
##HTD , starting from YTD4
#Cut to HTD columns
HTD1<-YTD4[,c(1,8:10)]
#Make numeric sort column, sort, erase sort column
HTD1$"sort3" <- as.numeric(sub("\\%","", HTD1$"LCG % pts. over OP"))
HTD2<-HTD1[order(-HTD1$sort3),]
rownames(HTD2)<-c(1:dim(HTD2)[1])
HTD3<-cbind("Rank"=rownames(HTD2),HTD2[,1:4])
#HTD Table ready.
###Fix non-displayed countries
YTD9<-YTD7
HTD4<-HTD3
View(YTD9)
regions <- function(YTD8) {
#UK Ireland
if (!is.null(which(YTD8[2]=="UK Ireland"))) {
YTD8[,2]<-gsub("UK Ireland", "Great Britain", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Great Britain"),])
YTD8[dim(YTD8)[1],2]<-"Ireland"
}
#Alpine = Swiss, Liechtenstein, Austria.
if (!is.null(which(YTD8[2]=="Alpine"))) {
YTD8[,2]<-gsub("Alpine", "Switzerland", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Switzerland"),])
YTD8[dim(YTD8)[1],2]<-"Liechtenstein"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Switzerland"),])
YTD8[dim(YTD8)[1],2]<-"Austria"
}
#Benelux = Belgium, Luxembourg, Netherlands
if (!is.null(which(YTD8[2]=="Benelux"))) {
YTD8[,2]<-gsub("Benelux", "Netherlands", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Netherlands"),])
YTD8[dim(YTD8)[1],2]<-"Luxembourg"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Netherlands"),])
YTD8[dim(YTD8)[1],2]<-"Belgium"
}
#Gulf = Kuwait, Bahrain, Oman, Qatar, UAE
if (!is.null(which(YTD8[2]=="Gulf"))) {
YTD8[,2]<-gsub("Gulf", "United Arab Emirates", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="United Arab Emirates"),])
YTD8[dim(YTD8)[1],2]<-"Oman"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="United Arab Emirates"),])
YTD8[dim(YTD8)[1],2]<-"Qatar"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="United Arab Emirates"),])
YTD8[dim(YTD8)[1],2]<-"Bahrain"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="United Arab Emirates"),])
YTD8[dim(YTD8)[1],2]<-"Kuwait"
}
#Iberia = Port, spain
if (!is.null(which(YTD8[2]=="Iberia"))) {
YTD8[,2]<-gsub("Iberia", "Spain", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Spain"),])
YTD8[dim(YTD8)[1],2]<-"Portugal"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Spain"),])
YTD8[dim(YTD8)[1],2]<-"Andorra"
}
#Central America & Caribbean Region = DR, Panama, Guatemala, CR, Honduras, Nicaragua, El Salvador
if (!is.null(which(YTD8[2]=="Central America & Caribbean Region"))) {
YTD8[,2]<-gsub("Central America & Caribbean Region", "Dominican Republic", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Panama"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Guatemala"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Costa Rica"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Honduras"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Nicaragua"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"El Salvador"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Jamaica"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Trinidad and Tobago"
}
#Andean Region = Ecuador, Peru, Bolivia, Paraguay
if (!is.null(which(YTD8[2]=="Andean Region"))) {
YTD8[,2]<-gsub("Andean Region", "Ecuador", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Ecuador"),])
YTD8[dim(YTD8)[1],2]<-"Peru"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Ecuador"),])
YTD8[dim(YTD8)[1],2]<-"Bolivia"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Ecuador"),])
YTD8[dim(YTD8)[1],2]<-"Paraguay"
}
#ANZ = NA for now
if (!is.null(which(YTD8[2]=="ANZ"))) {
YTD8[,2]<-gsub("ANZ", "Australia", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Australia"),])
YTD8[dim(YTD8)[1],2]<-"New Zealand"
}
#Nordic = NA for now
#Argentina Uruguay
#Cesko
#Singapore Region
return(YTD8)
}
YTD9<-regions(YTD9)
HTD4<-regions(HTD4)
View(YTD9)
shiny::runApp()
shiny::runApp()
shiny::runApp()
